# 데이터 불러오기
raw_twitter <- read.csv("crawl_data/twitter.csv", encoding="UTF-8")
library(dplyr)
library(readr)
library(textclean)
library(stringr)
library(tidytext)
library(ggplot2)
library(tidyr)
library(KoNLP)
library(RcppMeCab)
library(purrr)
setwd('C:/Users/admin/BigOne')
getwd()
raw_twitter <- read.csv("crawl_data/twitter.csv", encoding="UTF-8")
View(raw_twitter)
nrow(raw_twitter['Text']) # 3802
## 도배글 찾기
sum(duplicated(raw_twitter$Text)) # [1] 74
# 기본적인 전처리
raw_twitter3 <- raw_twitter2 %>%
mutate(id = row_number(), # id컬럼에 순서대로 행번호 넣기
Text = str_squish(replace_html(Text)),
Text = str_replace_all(Text, "[^가-힣]", " "),
Text = str_replace_all(Text, "[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣[:space:]]", " "))
raw_twitter2 <- raw_twitter
# 기본적인 전처리
raw_twitter3 <- raw_twitter2 %>%
mutate(id = row_number(), # id컬럼에 순서대로 행번호 넣기
Text = str_squish(replace_html(Text)),
Text = str_replace_all(Text, "[^가-힣]", " "),
Text = str_replace_all(Text, "[^0-9a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣[:space:]]", " "))
# 한글만 남기기
# str_squish : 문자열 공백문자를 단일 스페이스로 취급
# replace_html : HTML 마크업을 대체
View(raw_twitter3)
word_twitter <- raw_twitter3 %>%
# unnest_tokens : 다루고자하는 텍스트 데이터 객체
unnest_tokens(input = Text, # 정돈할 열이름
output = word, # 정돈된 결과열의 이름
token = SimplePos09, # "words" -> extractNoun 수정
drop = F)
View(word_twitter)
# 명사만 추출해서 형태소 정보를 제거함
word_twitter_n <- word_twitter %>%
filter(str_detect(word, "/n")) %>%
mutate(word = str_remove(word, "/.*$"))
View(word_twitter_n)
# 형용사도 추출
word_twitter_p <- word_twitter %>%
filter(str_detect(word, "/p")) %>%
mutate(word = str_replace_all(word, "/.*$", "다"))
View(word_twitter_p)
# 사용된 단어별로 count
word_twitter_done %>% count(word, sort=T) %>% View()
word_twitter_done <- bind_rows(word_twitter_n, word_twitter_p) %>%
arrange(id) %>%
filter(nchar(word) > 1) %>%
select(Datetime, Text, id, word)
View(word_twitter_done)
# 사용된 단어별로 count
word_twitter_done %>% count(word, sort=T) %>% View()
# 감정사전 정리된 완성본으로 다시 감정점수 분류
# Sys.setlocale("LC_ALL", "C") # - read_csv 에러 : invalid multibyte string, element 1
knu_dic2 <- read.csv("data/단어합본.csv", encoding="UTF-8")
Sys.setlocale("LC_ALL", "Korean")
# 감정사전 정리된 완성본으로 다시 감정점수 분류
# Sys.setlocale("LC_ALL", "C") # - read_csv 에러 : invalid multibyte string, element 1
knu_dic2 <- read.csv("data/단어합본.csv", encoding="UTF-8")
# 감정사전 정리된 완성본으로 다시 감정점수 분류
Sys.setlocale("LC_ALL", "C") # - read_csv 에러 : invalid multibyte string, element 1
knu_dic2 <- read.csv("data/단어합본.csv", encoding="UTF-8")
knu_dic2
knu_dic2 <- knu_dic2[-1] #첫번째 컬럼은 필요없어서 제외
head(knu_dic2)
# 감정 점수 부여하기
# dplyr::left_join() : 감정사전 word 기준 결합
# 없는단어는 polarity NA -> 0 처리
senti_word <- word_twitter_done %>%
left_join(knu_dic2, by = "word") %>%
filter(!is.na(polarity))
View(senti_word)
## polarity 점수별로 긍정/부정/중립 분류
senti_word2 <- senti_word %>%
mutate(sentiment = ifelse(polarity >= 1, "pos",
ifelse(polarity <= -1, "neg", "neu")))
View(senti_word2)
### 긍정, 부정단어별 가장 빈도가 많은 20개 추출
top10_senti <- senti_word2 %>%
filter(sentiment != "neu") %>%
count(sentiment, word) %>% filter(n > 1) %>%
group_by(sentiment) %>%
slice_max(n, n = 20)
# ------> 그래프
ggplot(top10_senti, aes(x = reorder(word, n),
y = n,
fill = sentiment)) +
geom_col() +
coord_flip() +
geom_text(aes(label = n), hjust = -0.3) +
facet_wrap(~ sentiment, scales = "free") +
scale_y_continuous(expand = expansion(mult = c(0.05, 0.15))) +
labs(x = NULL) +
ggtitle("트위터에서 가장 빈도가 많은 긍정,부정 단어 20개") +
theme(title = element_text(size=15, face='bold'))
